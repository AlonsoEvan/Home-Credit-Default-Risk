{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "# sklearn preprocessing for dealing with categorical variables\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "import os\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from lightgbm import LGBMClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bureau = pd.read_csv('train_bureau_raw.csv')\n",
    "test_bureau = pd.read_csv('test_bureau_raw.csv')\n",
    "\n",
    "train_previous = pd.read_csv('train_previous_raw.csv')\n",
    "test_previous = pd.read_csv('test_previous_raw.csv')\n",
    "\n",
    "# All columns in dataframes\n",
    "bureau_columns = list(train_bureau.columns)\n",
    "previous_columns = list(train_previous.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 123 original features.\n",
      "There are 179 bureau and bureau balance features.\n",
      "There are 1043 previous Home Credit loan features.\n"
     ]
    }
   ],
   "source": [
    "# Bureau only features\n",
    "bureau_features = list(set(bureau_columns) - set(previous_columns))\n",
    "\n",
    "# Previous only features\n",
    "previous_features = list(set(previous_columns) - set(bureau_columns))\n",
    "\n",
    "# Original features will be in both datasets\n",
    "original_features = list(set(previous_columns) & set(bureau_columns))\n",
    "\n",
    "print('There are %d original features.' % len(original_features))\n",
    "print('There are %d bureau and bureau balance features.' % len(bureau_features))\n",
    "print('There are %d previous Home Credit loan features.' % len(previous_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = train_bureau['TARGET']\n",
    "previous_features.append('SK_ID_CURR')\n",
    "\n",
    "train_ids = train_bureau['SK_ID_CURR']\n",
    "test_ids = test_bureau['SK_ID_CURR']\n",
    "\n",
    "# Merge the dataframes avoiding duplicating columns by subsetting train_previous\n",
    "train = train_bureau.merge(train_previous[previous_features], on = 'SK_ID_CURR')\n",
    "test = test_bureau.merge(test_previous[previous_features], on = 'SK_ID_CURR')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training shape:  (307511, 1345)\n",
      "Testing shape:  (48744, 1344)\n"
     ]
    }
   ],
   "source": [
    "print('Training shape: ', train.shape)\n",
    "print('Testing shape: ', test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training shape:  (1000, 1345)\n"
     ]
    }
   ],
   "source": [
    "train1 = train.sample(n = 1000, random_state = 42)\n",
    "\n",
    "print('Training shape: ', train1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_labels = train1['TARGET']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training shape:  (1000, 1344)\n"
     ]
    }
   ],
   "source": [
    "test1 = test.sample(n = 1000, random_state = 42)\n",
    "\n",
    "print('Training shape: ', test1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feat_sel(train, test):\n",
    "    \n",
    "    tr_labels = train['TARGET']\n",
    "    \n",
    "    train = train.drop(columns = ['SK_ID_CURR', 'TARGET'])\n",
    "    test = test.drop(columns = ['SK_ID_CURR'])\n",
    "    \n",
    "    print('Phase Label encoder')\n",
    "    #LABEL ENCODER\n",
    "    le = LabelEncoder()\n",
    "\n",
    "\n",
    "    # Iterate through the columns\n",
    "    for col in train:\n",
    "        if train[col].dtype == 'object':\n",
    "            # If 2 or fewer unique categories\n",
    "            if len(list(train[col].unique())) <= 2:\n",
    "                # Train on the training data\n",
    "                le.fit(train[col])\n",
    "                # Transform both training and testing data\n",
    "                train[col] = le.transform(train[col])\n",
    "                test[col] = le.transform(test[col])\n",
    "                \n",
    "    print('\\nDummyfication')\n",
    "    \n",
    "    #GET DUMMY\n",
    "    train = pd.get_dummies(train)\n",
    "    test = pd.get_dummies(test)\n",
    "    \n",
    "    #ALIGN\n",
    "    \n",
    "    train, test = train.align(test, join = 'inner', axis = 1)\n",
    "    \n",
    "    print('\\nNumber of feature in the training data after label encoder and get dummy: ', train.shape[1])\n",
    "    print('Number of feature in the testing data after label encoder and get dummy: ', test.shape[1])\n",
    "    \n",
    "    print('\\nPhase correlation')\n",
    "    \n",
    "    #Remove Collinear Variables\n",
    "    \n",
    "    tr = 0.9\n",
    "\n",
    "    corr = train.corr().abs()\n",
    "    \n",
    "    # Upper triangle of correlations\n",
    "    upper = corr.where(np.triu(np.ones(corr.shape), k=1).astype(np.bool))\n",
    "    \n",
    "    tr_drop = [columns for columns in upper.columns if any(upper[columns] > tr)]\n",
    "\n",
    "    print('\\nNumber of variable dropped because they were too correlated :',len(tr_drop))\n",
    "    \n",
    "    train1 = train.drop(columns = tr_drop)\n",
    "    test1 = test.drop(columns = tr_drop)\n",
    "\n",
    "    print('\\nNumber of feature in the training data after the drop of the variable too much correlated',train1.shape[1])\n",
    "    print('Number of feature in the testing data after the drop of the variable too much correlated',test1.shape[1])\n",
    "\n",
    "    print('\\nPhase Nan')\n",
    "    #Remove Missing Values\n",
    "    \n",
    "    train_missing = (train1.isnull().sum() / len(train1)).sort_values(ascending = False)\n",
    "    test_missing = (test1.isnull().sum() / len(test1)).sort_values(ascending = False)\n",
    "    \n",
    "    train_missing1 = train_missing.index[train_missing > 0.75]\n",
    "    test_missing1 = test_missing.index[test_missing > 0.75]\n",
    "\n",
    "    print('\\nNumber of columns with more than 75% of missing values in train :', len(train_missing1))\n",
    "    print('Number of columns with more than 75% of missing values in test :', len(test_missing1))\n",
    "    \n",
    "    train1 = train1.drop(columns = train_missing1 )\n",
    "    test1 = test1.drop(columns = test_missing1 )\n",
    "    \n",
    "    train, test = train1.align(test1, join = 'inner', axis = 1)\n",
    "    \n",
    "    print('\\nNumber of feature in the training data after removing missing values:', train.shape[1])\n",
    "    print('Number of feature in the testing data after removing missing values :', test.shape[1])\n",
    "    \n",
    "    \n",
    "    #MODELISATION\n",
    "    \n",
    "    coltrain = list(train.columns)\n",
    "    coltest = list(test.columns)\n",
    "\n",
    "    imputer = SimpleImputer(strategy = 'median')\n",
    "    \n",
    "    scaler = MinMaxScaler(feature_range = (0,1))\n",
    "\n",
    "    train = imputer.fit_transform(train)\n",
    "    test = imputer.transform(test)\n",
    "    \n",
    "    train = scaler.fit_transform(train)\n",
    "    test = scaler.transform(test)\n",
    "    \n",
    "    print('\\nStart of the feature selection with LGBM attributes : feature_importances_')\n",
    "    print('')\n",
    "    \n",
    "    zero_imp = np.zeros(train.shape[1])\n",
    "    \n",
    "    while(len(zero_imp) > 0 ):\n",
    "    \n",
    "        model = LGBMClassifier()\n",
    "\n",
    "        #fit the model twice to avoid overfitting\n",
    "        feat_imp = np.zeros(train.shape[1])\n",
    "    \n",
    "        for i in range(2):\n",
    "            X_train, X_valid, y_train, y_valid = train_test_split(train, tr_labels, test_size = 0.20, random_state = i)\n",
    "    \n",
    "            model.fit(X_train, y_train, early_stopping_rounds=100, \n",
    "                 eval_set = [(X_valid, y_valid)], eval_metric = 'auc', verbose = 200)\n",
    "    \n",
    "            feat_imp += model.feature_importances_\n",
    "    \n",
    "        feat_imp = feat_imp / 2\n",
    "\n",
    "        feat_imp = pd.DataFrame({'features' : coltrain, 'importances' : feat_imp}).sort_values(by = 'importances', ascending = False)\n",
    "    \n",
    "        zero_imp = list(feat_imp[feat_imp['importances'] == 0.0]['features'])\n",
    "\n",
    "        print('\\nThe number of features with 0.0 importance is:', len(zero_imp))\n",
    "        \n",
    "        train = pd.DataFrame(train, columns = coltrain)\n",
    "        test = pd.DataFrame(test, columns = coltest)\n",
    "\n",
    "        train.drop(columns = zero_imp, inplace = True)\n",
    "        test.drop(columns = zero_imp, inplace = True)\n",
    "\n",
    "        print(train.shape)\n",
    "        print(test.shape)\n",
    "        \n",
    "        coltrain = list(train.columns)\n",
    "        coltest = list(test.columns)\n",
    "\n",
    "        imputer = SimpleImputer(strategy = 'median')\n",
    "\n",
    "        train = imputer.fit_transform(train)\n",
    "        test = imputer.transform(test)\n",
    "        \n",
    "    print('\\n End of the features selection, we now have {} variables'.format(train.shape[1]))\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phase Label encoder\n",
      "\n",
      "Dummyfication\n",
      "\n",
      "Number of feature in the training data after label encoder and get dummy:  1441\n",
      "Number of feature in the testing data after label encoder and get dummy:  1441\n",
      "\n",
      "Phase correlation\n",
      "\n",
      "Number of variable dropped because they were too correlated : 614\n",
      "\n",
      "Number of feature in the training data after the drop of the variable too much correlated 827\n",
      "Number of feature in the testing data after the drop of the variable too much correlated 827\n",
      "\n",
      "Phase Nan\n",
      "\n",
      "Number of columns with more than 75% of missing values in train : 108\n",
      "Number of columns with more than 75% of missing values in test : 19\n",
      "\n",
      "Number of feature in the training data after removing missing values: 719\n",
      "Number of feature in the testing data after removing missing values : 719\n",
      "\n",
      "Start of the feature selection with LGBM attributes : feature_importances_\n",
      "\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[15]\tvalid_0's auc: 0.67964\tvalid_0's binary_logloss: 0.256482\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[68]\tvalid_0's auc: 0.757488\tvalid_0's binary_logloss: 0.375648\n",
      "\n",
      "The number of features with 0.0 importance is: 436\n",
      "(1000, 283)\n",
      "(1000, 283)\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[23]\tvalid_0's auc: 0.683964\tvalid_0's binary_logloss: 0.261229\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[68]\tvalid_0's auc: 0.757488\tvalid_0's binary_logloss: 0.375648\n",
      "\n",
      "The number of features with 0.0 importance is: 0\n",
      "(1000, 283)\n",
      "(1000, 283)\n",
      "\n",
      " End of the features selection, we now have 283 variables\n",
      "Wall time: 13.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train2, test2 = feat_sel(train1, test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 283)\n",
      "(1000, 283)\n"
     ]
    }
   ],
   "source": [
    "print(train2.shape)\n",
    "print(test2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phase Label encoder\n",
      "\n",
      "Dummyfication\n",
      "\n",
      "Number of feature in the training data after label encoder and get dummy:  1461\n",
      "Number of feature in the testing data after label encoder and get dummy:  1461\n",
      "\n",
      "Phase correlation\n",
      "\n",
      "Nombre de variable drop because they were too correlated : 594\n",
      "\n",
      "Number of feature in the training data after the drop of the variable too correlated 867\n",
      "Number of feature in the testing data after the drop of the variable too correlated 867\n",
      "\n",
      "Phase Nan\n",
      "\n",
      "Number of columns with more than 75% of missing values in train : 18\n",
      "Number of columns with more than 75% of missing values in test : 18\n",
      "Number of feature in the training data after removing missing values: 849\n",
      "Number of feature in the testing data after removing missing values : 849\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[98]\tvalid_0's auc: 0.780222\tvalid_0's binary_logloss: 0.236598\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[98]\tvalid_0's auc: 0.784045\tvalid_0's binary_logloss: 0.238869\n",
      "The number of features with 0.0 importance is: 335\n",
      "(307511, 514)\n",
      "(48744, 514)\n",
      "\n",
      "Number of 0 importances variables : 335\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[98]\tvalid_0's auc: 0.780222\tvalid_0's binary_logloss: 0.236598\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[98]\tvalid_0's auc: 0.784045\tvalid_0's binary_logloss: 0.238869\n",
      "The number of features with 0.0 importance is: 0\n",
      "(307511, 514)\n",
      "(48744, 514)\n",
      "\n",
      "Number of 0 importances variables : 0\n",
      "Wall time: 41min 18s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0, (307511, 514), (48744, 514))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "feat_sel(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
